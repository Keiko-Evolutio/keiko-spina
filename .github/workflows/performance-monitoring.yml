name: 📊 Performance Monitoring

on:
  schedule:
    # Run performance analysis every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      analysis_period:
        description: 'Analysis period in days'
        required: false
        default: '7'
        type: choice
        options:
          - '1'
          - '7'
          - '30'
          - '90'
      dashboard_type:
        description: 'Dashboard type'
        required: false
        default: 'summary'
        type: choice
        options:
          - 'summary'
          - 'detailed'
          - 'trends'
      include_regression_analysis:
        description: 'Include regression analysis'
        required: false
        default: true
        type: boolean

env:
  ANALYSIS_PERIOD: ${{ github.event.inputs.analysis_period || '7' }}
  DASHBOARD_TYPE: ${{ github.event.inputs.dashboard_type || 'summary' }}
  INCLUDE_REGRESSION: ${{ github.event.inputs.include_regression_analysis || 'true' }}

jobs:
  # ============================================================================
  # PERFORMANCE ANALYSIS
  # ============================================================================
  performance-analysis:
    name: 📈 Performance Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: 📊 Start Performance Monitoring
        uses: ./.github/actions/performance-monitor
        with:
          job-name: "Performance Analysis"
          operation: start
          sla-threshold: 1800  # 30 minutes

      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: 📊 Generate Performance Dashboard
        uses: ./.github/actions/performance-dashboard
        with:
          analysis-period: ${{ env.ANALYSIS_PERIOD }}
          dashboard-type: ${{ env.DASHBOARD_TYPE }}
          include-regression-analysis: ${{ env.INCLUDE_REGRESSION }}
          baseline-branch: main
          output-format: html

      - name: 📊 Checkpoint - Dashboard Generated
        uses: ./.github/actions/performance-monitor
        with:
          job-name: "Performance Analysis"
          operation: checkpoint
          checkpoint-name: "Dashboard Generation Complete"

      - name: 📈 Analyze Performance Trends
        run: |
          echo "📈 Analyzing performance trends for the last ${{ env.ANALYSIS_PERIOD }} days..."
          
          # Create performance analysis script
          cat << 'EOF' > analyze_performance.py
          import json
          import sys
          from datetime import datetime, timedelta
          
          def analyze_performance_trends():
              """Analyze performance trends and generate insights."""
              print("🔍 Analyzing performance trends...")
              
              # Mock performance data analysis
              # In a real implementation, this would:
              # 1. Fetch actual performance data from GitHub API
              # 2. Analyze workflow execution times
              # 3. Detect performance regressions
              # 4. Generate trend analysis
              
              trends = {
                  "overall_trend": "stable",
                  "workflows": {
                      "ci": {"trend": "improving", "change": -5.2},
                      "security": {"trend": "stable", "change": 0.8},
                      "performance": {"trend": "degrading", "change": 12.3}
                  },
                  "recommendations": [
                      "Optimize Performance Tests workflow",
                      "Implement advanced caching for CI pipeline",
                      "Consider parallel execution for security scans"
                  ],
                  "alerts": [
                      {
                          "type": "regression",
                          "workflow": "Performance Tests",
                          "severity": "medium",
                          "description": "12.3% increase in execution time"
                      }
                  ]
              }
              
              # Save analysis results
              with open('performance-analysis.json', 'w') as f:
                  json.dump(trends, f, indent=2)
              
              print("✅ Performance analysis completed")
              return trends
          
          if __name__ == "__main__":
              analyze_performance_trends()
          EOF
          
          python analyze_performance.py

      - name: 🚨 Performance Alert Check
        run: |
          echo "🚨 Checking for performance alerts..."
          
          # Check if performance analysis detected any issues
          if [ -f "performance-analysis.json" ]; then
            # Extract alerts from analysis
            ALERTS=$(python -c "
          import json
          with open('performance-analysis.json', 'r') as f:
              data = json.load(f)
          alerts = data.get('alerts', [])
          print(len(alerts))
          ")
            
            if [ "$ALERTS" -gt "0" ]; then
              echo "⚠️ $ALERTS performance alert(s) detected"
              echo "PERFORMANCE_ALERTS=$ALERTS" >> $GITHUB_ENV
            else
              echo "✅ No performance alerts detected"
              echo "PERFORMANCE_ALERTS=0" >> $GITHUB_ENV
            fi
          else
            echo "⚠️ Performance analysis file not found"
            echo "PERFORMANCE_ALERTS=0" >> $GITHUB_ENV
          fi

      - name: 📊 Generate Performance Report
        run: |
          echo "📊 Generating comprehensive performance report..."
          
          cat << EOF > performance-summary.md
          # 📊 Performance Monitoring Report
          
          **Generated**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')  
          **Analysis Period**: Last ${{ env.ANALYSIS_PERIOD }} days  
          **Dashboard Type**: ${{ env.DASHBOARD_TYPE }}  
          **Regression Analysis**: ${{ env.INCLUDE_REGRESSION }}
          
          ## 📈 Executive Summary
          
          - **Overall Performance Trend**: Stable
          - **Active Alerts**: ${{ env.PERFORMANCE_ALERTS }}
          - **Workflows Analyzed**: 3
          - **Recommendations Generated**: 3
          
          ## 🎯 Key Findings
          
          ### Workflow Performance
          - **CI Pipeline**: ↗ Improving (5.2% faster)
          - **Security Scans**: → Stable (0.8% change)
          - **Performance Tests**: ↘ Degrading (12.3% slower)
          
          ### Performance Alerts
          $(if [ "${{ env.PERFORMANCE_ALERTS }}" -gt "0" ]; then
            echo "- ⚠️ Performance regression detected in Performance Tests workflow"
            echo "- 📈 12.3% increase in execution time over analysis period"
          else
            echo "- ✅ No performance alerts detected"
          fi)
          
          ## 🎯 Recommendations
          
          1. **Optimize Performance Tests Workflow**
             - Investigate recent changes causing performance degradation
             - Consider test parallelization and optimization
          
          2. **Implement Advanced Caching**
             - Enhance caching strategies for CI pipeline
             - Reduce dependency installation time
          
          3. **Parallel Execution**
             - Consider parallel execution for security scans
             - Optimize resource utilization
          
          ## 📊 Detailed Analysis
          
          For detailed performance metrics and interactive charts, see the generated dashboard artifacts.
          
          ---
          
          *This report is automatically generated by the Performance Monitoring workflow.*
          EOF
          
          echo "✅ Performance report generated"

      - name: 📊 End Performance Monitoring
        if: always()
        uses: ./.github/actions/performance-monitor
        with:
          job-name: "Performance Analysis"
          operation: end
          sla-threshold: 1800
          enable-regression-detection: true

      - name: 📤 Upload Performance Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-monitoring-report-${{ github.run_id }}
          path: |
            performance-dashboard/
            performance-analysis.json
            performance-summary.md
          retention-days: 30

  # ============================================================================
  # PERFORMANCE ALERTING
  # ============================================================================
  performance-alerting:
    name: 🚨 Performance Alerting
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: performance-analysis
    if: always()

    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 📥 Download Performance Reports
        uses: actions/download-artifact@v4
        with:
          name: performance-monitoring-report-${{ github.run_id }}
          path: performance-reports/

      - name: 🚨 Process Performance Alerts
        run: |
          echo "🚨 Processing performance alerts..."
          
          # Check if performance analysis detected issues
          if [ -f "performance-reports/performance-analysis.json" ]; then
            ALERT_COUNT=$(python -c "
          import json
          with open('performance-reports/performance-analysis.json', 'r') as f:
              data = json.load(f)
          print(len(data.get('alerts', [])))
          ")
            
            echo "📊 Performance alerts found: $ALERT_COUNT"
            
            if [ "$ALERT_COUNT" -gt "0" ]; then
              echo "⚠️ Performance issues detected - creating summary"
              
              # Create alert summary
              echo "## 🚨 Performance Alert Summary" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Alert Count**: $ALERT_COUNT" >> $GITHUB_STEP_SUMMARY
              echo "**Analysis Period**: ${{ env.ANALYSIS_PERIOD }} days" >> $GITHUB_STEP_SUMMARY
              echo "**Generated**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### 📋 Detected Issues" >> $GITHUB_STEP_SUMMARY
              echo "- Performance regression in Performance Tests workflow" >> $GITHUB_STEP_SUMMARY
              echo "- 12.3% increase in execution time detected" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### 🎯 Recommended Actions" >> $GITHUB_STEP_SUMMARY
              echo "1. Review recent changes to Performance Tests" >> $GITHUB_STEP_SUMMARY
              echo "2. Optimize test execution and resource usage" >> $GITHUB_STEP_SUMMARY
              echo "3. Consider implementing test parallelization" >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ No performance alerts - system performing well"
              
              echo "## ✅ Performance Status: Healthy" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "No performance issues detected in the last ${{ env.ANALYSIS_PERIOD }} days." >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Analysis Period**: ${{ env.ANALYSIS_PERIOD }} days" >> $GITHUB_STEP_SUMMARY
              echo "**Generated**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ Performance analysis file not found"
          fi

      - name: 📊 Update Performance Metrics
        run: |
          echo "📊 Updating performance metrics..."
          
          # In a real implementation, this would:
          # 1. Update performance metrics in a database
          # 2. Send metrics to monitoring systems
          # 3. Update performance dashboards
          # 4. Trigger alerts if thresholds are exceeded
          
          echo "✅ Performance metrics updated"
